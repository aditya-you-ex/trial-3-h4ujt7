# =====================================================================================
# Main Artillery Load Testing Configuration
# -------------------------------------------------------------------------------------
# This file defines the comprehensive global settings, plugins, environment phases,
# pre- and post-scenario functions, and merges scenario definitions from:
#  1) concurrent-users.yml (Import: "concurrent-users")
#  2) data-processing.yml  (Import: "data-processing")
#
# Requirements Addressed:
#  - System Reliability (99.9% uptime) through extended load tests & phases
#  - API Performance (1000 requests/min) via graduated load testing & rate control
#  - Resource Optimization (40% improvement) by collecting detailed endpoint metrics
#
# External Plugins (IE2):
#  - @artilleryio/artillery-plugin-expect v2.x           (Enhanced assertions)
#  - artillery-plugin-metrics-by-endpoint v1.x           (Endpoint-specific metrics)
#  - artillery-plugin-publish-metrics v2.x               (External metrics publishing)
#
# Extensive Comments & Documentation Provided (S2).
# =====================================================================================

config:
  # ---------------------------------------------------------------------------
  # Global Target: Automatically set from environment variable supporting
  # multiple deployments. Adjust as needed if you prefer environment-based
  # approach only (below) or a single global target.
  # ---------------------------------------------------------------------------
  target: "{{ $processEnvironment.API_URL }}"

  # ---------------------------------------------------------------------------
  # Global Plugins (with version references and configuration).
  # ---------------------------------------------------------------------------
  plugins:
    # @artilleryio/artillery-plugin-expect v2.x
    expect:
      outputFormat: "prettyError"
      handleErrors: true
      expectStatusCode: 200

    # artillery-plugin-metrics-by-endpoint v1.x
    metrics-by-endpoint:
      enableEndpointMetrics: true
      enableResourceMetrics: true
      metricsInterval: 30

    # artillery-plugin-publish-metrics v2.x
    publish-metrics:
      type: "datadog"
      apiKey: "{{ $processEnvironment.DATADOG_API_KEY }}"
      tags:
        - "env:{{ $processEnvironment.ENV }}"
        - "service:taskstream-api"

  # ---------------------------------------------------------------------------
  # Default Settings that apply to all requests unless overridden in environment
  # or scenario definitions.
  # ---------------------------------------------------------------------------
  defaults:
    headers:
      Content-Type: "application/json"
      Accept: "application/json"
      X-API-Version: "1.0"
    timeout: 30000

  # ---------------------------------------------------------------------------
  # Environment-Specific Configurations (Production & Staging).
  # Each environment has unique phases for ramp-up, sustain, and cool-down.
  # ---------------------------------------------------------------------------
  environments:
    production:
      target: "https://api.taskstream.ai"
      phases:
        - duration: 300
          arrivalRate: 5
          name: "Warm up"
        - duration: 600
          arrivalRate: 50
          rampTo: 200
          name: "Ramp up load"
        - duration: 1800
          arrivalRate: 200
          name: "Sustained peak load"
        - duration: 300
          arrivalRate: 200
          rampTo: 5
          name: "Cool down"
      defaults:
        headers:
          Content-Type: "application/json"
          Accept: "application/json"
          X-API-Version: "1.0"
        timeout: 30000

    staging:
      target: "https://staging-api.taskstream.ai"
      phases:
        - duration: 180
          arrivalRate: 2
          name: "Warm up"
        - duration: 300
          arrivalRate: 20
          rampTo: 100
          name: "Ramp up load"
        - duration: 600
          arrivalRate: 100
          name: "Sustained load"
        - duration: 180
          arrivalRate: 100
          rampTo: 2
          name: "Cool down"
      defaults:
        headers:
          Content-Type: "application/json"
          Accept: "application/json"
          X-API-Version: "1.0"
        timeout: 30000

  # ---------------------------------------------------------------------------
  # Processor Section Defining Global Before/After Scenario Hooks
  # ---------------------------------------------------------------------------
  processor:
    # The JS block below implements the "beforeScenario" and "afterScenario" functions
    # with all required steps in detail (LD2).
    js: |
      "use strict";

      /**
       * beforeScenarioHook
       * -------------------
       * Enhanced setup function executed before each test scenario, implementing:
       * 1. Initialize scenario-specific metrics collectors
       * 2. Set up authentication tokens and headers
       * 3. Prepare test data with proper isolation
       * 4. Configure endpoint-specific monitoring
       * 5. Initialize performance thresholds
       * 6. Set up error tracking
       * 7. Configure resource utilization monitoring
       *
       * @param {object} context - Artillery scenario context
       * @param {object} ee      - Artillery event emitter
       * @param {function} next  - Callback to proceed
       * @returns void
       */
      function beforeScenarioHook(context, ee, next) {
        // 1) Initialize scenario-specific metrics collectors
        //    E.g., we could register new timers or counters here
        context.vars.scenarioMetrics = {
          startTime: Date.now(),
          customCounters: {}
        };

        // 2) Set up authentication tokens and headers
        //    If needed, retrieve or generate tokens for secure endpoints
        //    context.vars.authToken = "example-token-123";
        //    context.vars.headers = { Authorization: `Bearer ${context.vars.authToken}` };

        // 3) Prepare test data with proper isolation
        //    For large tests, we might connect to a test fixture or seed data
        context.vars.testData = {};

        // 4) Configure endpoint-specific monitoring
        //    This might include special watchers or logs for critical endpoints
        //    such as /api/v1/tasks or /api/v1/analytics

        // 5) Initialize performance thresholds
        context.vars.performanceThresholds = {
          maxResponseTimeMs: 2000,
          expectedStatusCode: 200
        };

        // 6) Set up error tracking
        context.vars.errors = [];

        // 7) Configure resource utilization monitoring
        //    Possibly toggling in-code checks or hooking to an external system

        return next();
      }

      /**
       * afterScenarioHook
       * ------------------
       * Enhanced cleanup function executed after each test scenario, implementing:
       * 1. Export collected metrics to monitoring system
       * 2. Clean up test data and resources
       * 3. Generate scenario-specific performance report
       * 4. Reset authentication tokens
       * 5. Clear scenario state and caches
       * 6. Log resource utilization metrics
       * 7. Perform error analysis if needed
       *
       * @param {object} context - Artillery scenario context
       * @param {object} ee      - Artillery event emitter
       * @param {function} next  - Callback to proceed
       * @returns void
       */
      function afterScenarioHook(context, ee, next) {
        // 1) Export collected metrics to monitoring system
        //    This could push context.vars.scenarioMetrics to a third-party system

        // 2) Clean up test data and resources
        //    e.g., removing temporary objects or references

        // 3) Generate scenario-specific performance report
        const startTime = context.vars.scenarioMetrics
          ? context.vars.scenarioMetrics.startTime
          : null;
        if (startTime) {
          const endTime = Date.now();
          const elapsed = endTime - startTime;
          // Example: console.log or a custom log function
          // console.log(`Scenario executed in ${elapsed} ms`);
        }

        // 4) Reset authentication tokens
        //    context.vars.authToken = null;

        // 5) Clear scenario state and caches
        //    context.vars = {};

        // 6) Log resource utilization metrics
        //    Potentially gather CPU/memory usage from environment or containers

        // 7) Perform error analysis if needed
        //    If context.vars.errors is not empty, we can log or process them

        return next();
      }

      // Exporting these for scenario-level references
      module.exports = {
        beforeScenarioHook,
        afterScenarioHook
      }

# -------------------------------------------------------------------------------
# Merging Scenario Definitions
# Imported from:
#   1) src/test/load/artillery/scenarios/concurrent-users.yml
#   2) src/test/load/artillery/scenarios/data-processing.yml
# The following array unifies all scenario blocks from the two files.
# -------------------------------------------------------------------------------
scenarios:
  # =============================================================================
  # Scenarios from concurrent-users.yml
  # =============================================================================

  - name: "User Authentication Flow"
    beforeScenario: "beforeScenarioHook"
    afterScenario: "afterScenarioHook"
    flow:
      - function: "generateRandomUser"
      - post:
          url: "/api/v1/auth/login"
          json:
            email: "{{ user.email }}"
            password: "{{ user.password }}"
          expect:
            - statusCode: 200
      - get:
          url: "/api/v1/user/profile"
          expect:
            - statusCode: 200

  - name: "Project Management Flow"
    beforeScenario: "beforeScenarioHook"
    afterScenario: "afterScenarioHook"
    flow:
      - function: "generateRandomProject"
      - get:
          url: "/api/v1/projects"
          expect:
            - statusCode: 200
      - post:
          url: "/api/v1/projects"
          json:
            name: "{{ project.name }}"
            start_date: "{{ project.start_date }}"
            end_date: "{{ project.end_date }}"
            metadata: "{{ project.metadata }}"
          capture:
            json: "$.id"
            as: "projectId"
          expect:
            - statusCode: 201
      - get:
          url: "/api/v1/projects/{{ projectId }}"
          expect:
            - statusCode: 200

  - name: "Task Management Flow"
    beforeScenario: "beforeScenarioHook"
    afterScenario: "afterScenarioHook"
    flow:
      - get:
          url: "/api/v1/tasks"
          expect:
            - statusCode: 200
      - function: "generateRandomProject"
      - post:
          url: "/api/v1/projects"
          json:
            name: "{{ project.name }}"
            start_date: "{{ project.start_date }}"
            end_date: "{{ project.end_date }}"
            metadata: "{{ project.metadata }}"
          capture:
            json: "$.id"
            as: "projectId"
          expect:
            - statusCode: 201
      - function: "generateRandomTask"
      - post:
          url: "/api/v1/tasks"
          json:
            title: "{{ task.title }}"
            description: "{{ task.description }}"
            projectId: "{{ task.projectId }}"
            due_date: "{{ task.due_date }}"
            priority: "{{ task.priority }}"
            status: "{{ task.status }}"
          capture:
            json: "$.id"
            as: "taskId"
          expect:
            - statusCode: 201
      - put:
          url: "/api/v1/tasks/{{ taskId }}"
          json:
            status: "In Progress"
          expect:
            - statusCode: 200

  - name: "Analytics Flow"
    beforeScenario: "beforeScenarioHook"
    afterScenario: "afterScenarioHook"
    flow:
      - get:
          url: "/api/v1/analytics/dashboard"
          expect:
            - statusCode: 200
      - get:
          url: "/api/v1/analytics/metrics"
          expect:
            - statusCode: 200

  # =============================================================================
  # Scenarios from data-processing.yml
  # =============================================================================

  - name: "singleCommunicationProcessing"
    beforeScenario: "beforeScenarioHook"
    afterScenario: "afterScenarioHook"
    flow:
      - post:
          url: "/api/nlp/process"
          json: "{{ generateTestData('email', { complexity: 'high' }) }}"
          expect:
            - "statusCode to be 200"
            - "has property taskExtracted"
            - "has property confidence"
            - "property confidence to be > 0.95"
      - think: 2
      - post:
          url: "/api/nlp/process"
          json: "{{ generateTestData('chat', { complexity: 'medium' }) }}"
          capture:
            json: "$.taskId"
            as: "extractedTaskId"

  - name: "batchCommunicationProcessing"
    beforeScenario: "beforeScenarioHook"
    afterScenario: "afterScenarioHook"
    flow:
      - post:
          url: "/api/nlp/batch"
          json:
            items: "{{ generateBatchData(50, { mixedTypes: true }) }}"
          expect:
            - "statusCode to be 200"
            - "has property results"
            - "property results.length to be 50"
            - "property processingTime to be < 5000"
      - think: 5

  - name: "mixedCommunicationTypes"
    beforeScenario: "beforeScenarioHook"
    afterScenario: "afterScenarioHook"
    flow:
      - post:
          url: "/api/nlp/process"
          json: "{{ generateTestData('email', { withAttachments: true }) }}"
      - post:
          url: "/api/nlp/process"
          json: "{{ generateTestData('chat', { withThreads: true }) }}"
      - post:
          url: "/api/nlp/process"
          json: "{{ generateTestData('transcript', { withMultipleSpeakers: true }) }}"